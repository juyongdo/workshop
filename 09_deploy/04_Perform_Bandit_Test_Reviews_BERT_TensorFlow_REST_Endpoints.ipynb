{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits and Reinforcement Learning with Amazon SageMaker\n",
    "\n",
    "We demonstrate how you can manage your own contextual multi-armed bandit workflow on SageMaker using the built-in [AWS Reinforcement Learning Container](https://github.com/aws/sagemaker-rl-container) container to train and deploy contextual bandit models. We show how to train these models that interact with a live environment (using a simulated client application) and continuously update the model with efficient exploration.\n",
    "\n",
    "### Why Contextual Bandits?\n",
    "\n",
    "Wherever we look to personalize content for a user (content layout, ads, search, product recommendations, etc.), contextual bandits come in handy. Traditional personalization methods collect a training dataset, build a model and deploy it for generating recommendations. However, the training algorithm does not inform us on how to collect this dataset, especially in a production system where generating poor recommendations lead to loss of revenue. Contextual bandit algorithms help us collect this data in a strategic manner by trading off between exploiting known information and exploring recommendations which may yield higher benefits. The collected data is used to update the personalization model in an online manner. Therefore, contextual bandits help us train a personalization model while minimizing the impact of poor recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/multi_armed_bandit_maximize_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the exploration-exploitation strategy, we need an iterative training and deployment system that: (1) recommends an action using the contextual bandit model based on user context, (2) captures the implicit feedback over time and (3) continuously trains the model with incremental interaction data. In this notebook, we show how to setup the infrastructure needed for such an iterative learning system. While the example demonstrates a bandits application, these continual learning systems are useful more generally in dynamic scenarios where models need to be continually updated to capture the recent trends in the data (e.g. tracking fraud behaviors based on detection mechanisms or tracking user interests over time). \n",
    "\n",
    "In a typical supervised learning setup, the model is trained with a SageMaker training job and it is hosted behind a SageMaker hosting endpoint. The client application calls the endpoint for inference and receives a response. In bandits, the client application also sends the reward (a score assigned to each recommendation generated by the model) back for subsequent model training. These rewards will be part of the dataset for the subsequent model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant Links\n",
    "\n",
    "In-Practice\n",
    "* [AWS Blog Post on Contextual Multi-Armed Bandits](https://aws.amazon.com/blogs/machine-learning/power-contextual-bandits-using-continual-learning-with-amazon-sagemaker-rl/)\n",
    "* [Multi-Armed Bandits at StitchFix](https://multithreaded.stitchfix.com/blog/2020/08/05/bandits/)\n",
    "* [Introduction to Contextual Bandits](https://getstream.io/blog/introduction-contextual-bandits/)\n",
    "* [Vowpal Wabbit Contextual Bandit Algorithms](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Contextual-Bandit-algorithms)\n",
    "\n",
    "Theory\n",
    "* [Learning to Interact](https://hunch.net/~jl/interact.pdf)\n",
    "* [Contextual Bandit Bake-Off](https://arxiv.org/pdf/1802.04064.pdf)\n",
    "* [Doubly-Robust Policy Evaluation and Learning](https://arxiv.org/pdf/1103.4601.pdf)\n",
    "\n",
    "Code\n",
    "* [AWS Open Source Reinforcement Learning Containers](https://github.com/aws/sagemaker-rl-container)\n",
    "* [AWS Open Source Bandit Experiment Manager](./common/sagemaker_rl/orchestrator/workflow/manager)\n",
    "* [Vowpal Wabbit Reinforcement Learning Framework](https://github.com/VowpalWabbit/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Open Source Bandit `ExperimentManager` Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/multi_armed_bandit_traffic_shift.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bandit model is implemented by the open source [**Bandit Experiment Manager**](./common/sagemaker_rl/orchestrator/workflow/manager/) provided with this example.  This This implementation continuously updates a Vowpal Wabbit reinforcement learning model using Amazon SageMaker, DynamoDB, Kinesis, and S3.\n",
    "\n",
    "The client application, a recommender system with a review service in our case, pings the SageMaker hosting endpoint that is serving the bandit model.  The application sends the an `event` with the `context` (ie. user, product, and review text) to the bandit model and receives a recommended action from the bandit model.  In our case, the action is 1 of 2 BERT models that we are testing.  The bandit model stores this event data (given context and recommended action) in S3 using Amazon Kinesis.  _Note:  The context makes this a \"contextual bandit\" and differentiates this implementation from a regular multi-armed bandit._\n",
    "\n",
    "The client application uses the recommended BERT model to classify the review text as star rating 1 through 5 and  compares the predicted star rating to the user-selected star rating.  If the BERT model correctly predicts the star rating of the review text (ie. matches the user-selected star rating), then the bandit model is rewarded with `reward=1`.  If the BERT model incorrectly classifies the star rating of the review text, the bandit model is not rewarded (`reward=0`).\n",
    "\n",
    "The client application stores the rewards data in S3 using Amazon Kinesis.  Periodically (ie. every 100 rewards), we incrementally train an updated bandit model with the latest the reward and event data.  This updated bandit model is evaluated against the current model using a holdout dataset of rewards and events.  If the bandit model accuracy is above a given threshold relative to the existing model, it is automatically deployed in a blue/green manner with no downtime.  SageMaker RL supports offline evaluation by performing counterfactual analysis (CFA).  By default, we apply [**doubly robust (DR) estimation**](https://arxiv.org/pdf/1103.4601.pdf) method. The bandit model tries to minimize the cost (`1 - reward`), so a smaller evaluation score indicates better bandit model performance.\n",
    "\n",
    "Unlike traditional A/B tests, the bandit model will learn the best BERT model (action) for a given context over time and begin to shift traffic to the best model.  Depending on the aggressiveness of the bandit model algorithm selected, the bandit model will continuously explore the under-performing models, but start to favor and exploit the over-performing models.  And unlike A/B tests, multi-armed bandits allow you to add a new action (ie. BERT model) dynamically throughout the life of the experiment.  When the bandit model sees the new BERT model, it will start sending traffic and exploring the accuracy of the new BERT model - alongside the existing BERT models in the experiment.\n",
    "\n",
    "#### Local Mode\n",
    "\n",
    "To facilitate experimentation, we provide a `local_mode` that runs the contextual bandit example using the SageMaker Notebook instance itself instead of the SageMaker training and hosting cluster instances.  The workflow remains the same in `local_mode`, but runs much faster for small datasets.  Hence, it is a useful tool for experimenting and debugging.  However, it will not scale to production use cases with high throughput and large datasets.  In `local_mode`, the training, evaluation, and hosting is done in the local [SageMaker Vowpal Wabbit Docker Container](https://github.com/aws/sagemaker-rl-container)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import sagemaker\n",
    "\n",
    "sys.path.append('common')\n",
    "sys.path.append('common/sagemaker_rl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "The configuration for the bandits application can be specified in a `config.yaml` file as can be seen below. It configures the AWS resources needed. The DynamoDB tables are used to store metadata related to experiments, models and data joins. The `private_resource` specifices the SageMaker instance types and counts used for training, evaluation and hosting. The SageMaker container image is used for the bandits application. This config file also contains algorithm and SageMaker-specific setups.  Note that all the data generated and used for the bandits application will be stored in `s3://sagemaker-{REGION}-{AWS_ACCOUNT_ID}/{experiment_id}/`.\n",
    "\n",
    "Please make sure that the `num_arms` parameter in the config is equal to the number of actions in the client application (which is defined in the cell below).\n",
    "\n",
    "The Docker image is defined here:  https://github.com/aws/sagemaker-rl-container/blob/master/vw/docker/8.7.0/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pygmentize 'config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config.yaml'\n",
    "with open(config_file, 'r') as yaml_file:\n",
    "    config = yaml.load(yaml_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional permissions for the IAM role\n",
    "IAM role requires additional permissions for [AWS CloudFormation](https://aws.amazon.com/cloudformation/), [Amazon DynamoDB](https://aws.amazon.com/dynamodb/), [Amazon Kinesis Data Firehose](https://aws.amazon.com/kinesis/data-firehose/) and [Amazon Athena](https://aws.amazon.com/athena/). Make sure the SageMaker role you are using has the permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from markdown_helper import *\n",
    "# from IPython.display import Markdown\n",
    "\n",
    "# display(Markdown(generate_help_for_experiment_manager_permissions(role)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client Application (Environment)\n",
    "The client application simulates a live environment that uses the bandit model to recommend a BERT model to classify review text submitted by the application user. \n",
    "\n",
    "The logic of reward generation resides in the client application.  We simulate the online learning loop with feedback.  The data consists of 2 actions - 1 for each BERT model under test.  If the bandit model selects the right class, then the model is rewarded with `reward=1`.  Otherwise, the bandit model receives `reward=0`.\n",
    "\n",
    "The workflow of the client application is as follows:\n",
    "- Our client application picks sample review text at random, which is sent to the bandit model (SageMaker endpoint) to recommend an action (BERT model) to classify the review text into star rating 1 through 5.\n",
    "- The bandit model returns an action, an action probability, and an `event_id` for this prediction event.\n",
    "- Since the client application uses the Amazon Customer Reviews Dataset, we know the true star rating for the review text\n",
    "- The client application compares the predicted and true star rating and assigns a reward to the bandit model using Amazon Kinesis, S3, and DynamoDB.  (The `event_id` is used to join the event and reward data.)\n",
    "\n",
    "`event_id` is a unique identifier for each interaction. It is used to join inference data `<state, action, action_probability>` with the reward data. \n",
    "\n",
    "In a later cell of this notebook, we illustrate how the client application interacts with the bandit model endpoint and receives the recommended action (BERT model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step bandits model development\n",
    "\n",
    "[**Bandit Experiment Manager**](./common/sagemaker_rl/orchestrator/workflow/manager/) is the top level class for all the Bandits/RL and continual learning workflows. Similar to the estimators in the [Sagemaker Python SDK](https://github.com/aws/sagemaker-python-sdk), `ExperimentManager` contains methods for training, deployment and evaluation. It keeps track of the job status and reflects current progress in the workflow.\n",
    "\n",
    "Start the application using the `ExperimentManager` class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = int(time.time())\n",
    "\n",
    "experiment_name = 'bandits-{}'.format(timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ExperimentManager` will create a AWS CloudFormation Stack of additional resources needed for the Bandit experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestrator.workflow.manager.experiment_manager import ExperimentManager\n",
    "\n",
    "bandit_experiment_manager = ExperimentManager(config, experiment_id=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bandit_experiment_manager.clean_resource(experiment_id=bandit_experiment_manager.experiment_id)\n",
    "    bandit_experiment_manager.clean_table_records(experiment_id=bandit_experiment_manager.experiment_id)\n",
    "except:\n",
    "    print('Ignore any errors.  Errors are OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_experiment_manager = ExperimentManager(config, experiment_id=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Bandit Model\n",
    "To start a new experiment, we need to initialize the first bandit model or \"policy\" in reinforcement learning terminology.  \n",
    "\n",
    "If we have historical data in the format `(state, action, action probability, reward)`, we can perform a \"warm start\" and learn the bandit model offline.  \n",
    "\n",
    "However, let's assume we are starting with no historical data and initialize a random bandit model using `initialize_first_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit_experiment_manager.initialize_first_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ^^ Ignore `Failed to delete: /tmp/...` message above.  This is OK. ^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Experiment State:  TRAINED\n",
    "`training_state`: `TRAINED`\n",
    "\n",
    "Remember the `last_trained_model_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_experiment_manager._jsonify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Bandit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training and evaluation is done, we can deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the model_id of the last model trained.\n",
    "print('Deploying newly-trained bandit model: {}'.format(bandit_experiment_manager.last_trained_model_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Deploying bandit model_id {}'.format(bandit_experiment_manager.last_trained_model_id))\n",
    "\n",
    "bandit_experiment_manager.deploy_model(model_id=bandit_experiment_manager.last_trained_model_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Experiment State\n",
    "`hosting_state`: `DEPLOYED`\n",
    "\n",
    "The `last_trained_model_id` and `last_hosted_model_id` are now the same as we just deployed the bandit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit_experiment_manager._jsonify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the 2 BERT Models to Test with our Bandit Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the last trained bandit model is deployed as a SageMaker Endpoint, the client application will send the context to the endpoint and receive the recommended action.  The bandit model will recommend 1 of 2 actions in our example:  `1` or `2` which correspond to BERT model 1 and BERT model 2, respectively.  Let's configure these 2 BERT models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r tensorflow_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_endpoint_name = tensorflow_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_1_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker')\n",
    "waiter = client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=model_1_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.tensorflow.serving import Predictor\n",
    "\n",
    "model1 = Predictor(endpoint_name=model_1_endpoint_name,\n",
    "                   sagemaker_session=sess,\n",
    "                   content_type='application/json',\n",
    "                   model_name='saved_model',\n",
    "                   model_version=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['This is great!']\n",
    "\n",
    "model1_predicted_classes = model1.predict(reviews)\n",
    "\n",
    "for predicted_class, review in zip(model1_predicted_classes, reviews):\n",
    "    print('[Predicted Star Rating: {}]'.format(predicted_class), review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">Model 1 SageMaker REST Endpoint</a></b>'.format(region, model_1_endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pytorch_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_endpoint_name = pytorch_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_2_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = sm.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=model_2_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "model2 = RealTimePredictor(endpoint=model_2_endpoint_name,\n",
    "                           sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = 'This is great!'\n",
    "\n",
    "model2_predicted_class = model2.predict(reviews).decode('utf-8')\n",
    "\n",
    "print(model2_predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">Model 2 SageMaker REST Endpoint</a></b>'.format(region, model_2_endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Client Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "class ClientApp():\n",
    "    def __init__(self, data, num_events, bandit_model, bert_model_map):\n",
    "        self.bandit_model = bandit_model\n",
    "        self.bert_model_map = bert_model_map\n",
    "        \n",
    "        self.num_actions = 2\n",
    "\n",
    "        df_reviews = pd.read_csv(data, \n",
    "                                 delimiter='\\t', \n",
    "                                 quoting=csv.QUOTE_NONE,\n",
    "                                 compression='gzip')\n",
    "        df_scrubbed = df_reviews[['review_body', 'star_rating']].sample(n=num_events) # .query('star_rating == 1')\n",
    "        df_scrubbed = df_scrubbed.reset_index()\n",
    "        df_scrubbed.shape\n",
    "        np_reviews = df_scrubbed.to_numpy()\n",
    "\n",
    "        np_reviews = np.delete(np_reviews, 0, 1)\n",
    "        \n",
    "        # Last column is the label, the rest are the features (contexts)\n",
    "        self.labels = np_reviews[:, -1]\n",
    "        self.contexts = np_reviews[:, :-1].tolist()\n",
    "\n",
    "        self.optimal_rewards = [1]\n",
    "        self.rewards_buffer = []\n",
    "        self.joined_data_buffer = []\n",
    "\n",
    "    def choose_random_context(self):\n",
    "#        context_index = np.random.choice(self.contexts.shape[0])\n",
    "        context_index = np.random.choice(len(self.contexts))\n",
    "        context = self.contexts[context_index]\n",
    "        return context_index, context    \n",
    "\n",
    "    def clear_buffer(self):\n",
    "        self.rewards_buffer.clear()\n",
    "        self.joined_data_buffer.clear()  \n",
    "\n",
    "    def get_reward(self, \n",
    "                   context_index, \n",
    "                   action, \n",
    "                   event_id, \n",
    "                   bandit_model_id, \n",
    "                   action_prob, \n",
    "                   sample_prob, \n",
    "                   local_mode):\n",
    "\n",
    "        context_to_predict = self.contexts[context_index][0]\n",
    "#        print('Context to predict{}'.format(context_to_predict))\n",
    "        print('Context index {}'.format(context_index))\n",
    "    \n",
    "        label = self.labels[context_index]\n",
    "        print('Label {}'.format(label))\n",
    "        \n",
    "        print('Action {}'.format(action))\n",
    "        \n",
    "        bert_model = self.bert_model_map[action]\n",
    "        try: \n",
    "            # TensorFlow takes a list and returns str\n",
    "            bert_predicted_class = bert_model.predict([context_to_predict])[0]\n",
    "            bert_predicted_class #.decode('utf-8')\n",
    "            print('Predicted Class from Model 1 {}'.format(bert_predicted_class))        \n",
    "        except:\n",
    "            # PyTorch takes a single review and returns bytes\n",
    "            #context = context_index            \n",
    "            bert_predicted_class = bert_model.predict(context_to_predict)[0]\n",
    "            bert_predicted_class = bert_predicted_class #.decode('utf-8')\n",
    "            print('Predicted Class from Model 2 {}'.format(bert_predicted_class))\n",
    "        \n",
    "        # Calculate difference between predicted and actual label\n",
    "        if abs(int(bert_predicted_class) - int(label)) <= 1:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        if local_mode:\n",
    "            json_blob = {\"reward\": reward,\n",
    "                         \"event_id\": event_id,\n",
    "                         \"action\": action,\n",
    "                         \"action_prob\": action_prob,\n",
    "                         \"model_id\": bandit_model_id,\n",
    "                         \"observation\": [context_index],\n",
    "                         \"sample_prob\": sample_prob}\n",
    "            \n",
    "            self.joined_data_buffer.append(json_blob)\n",
    "        else:\n",
    "            json_blob = {\"reward\": reward, \"event_id\": event_id}\n",
    "            self.rewards_buffer.append(json_blob)\n",
    "        \n",
    "        return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_model = bandit_experiment_manager.predictor\n",
    "\n",
    "client_app = ClientApp(data='./data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz',\n",
    "                       num_events=100,\n",
    "                       bandit_model=bandit_model,\n",
    "                       bert_model_map={\n",
    "                         1: model1,\n",
    "                         2: model2\n",
    "                       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `num_arms` specified in `config.yaml` is equal to the total unique actions in the simulation application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing {} BERT models'.format(client_app.num_actions))\n",
    "\n",
    "assert client_app.num_actions == bandit_experiment_manager.config[\"algor\"][\"algorithms_parameters\"][\"num_arms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "context_index, context = client_app.choose_random_context()\n",
    "action, event_id, bandit_model_id, action_prob, sample_prob = bandit_model.get_action(obs=[context_index])\n",
    "\n",
    "print('event ID: {}\\nbert_model_id: {}\\naction_probability: {}'.format(event_id, action, action_prob, bandit_model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sample Events to Test the Bandit `ExperimentManager`\n",
    "Thsi will generated sample contexts to pass as events to the bandit using the Amazon Customer Reviews Dataset.  The bandit model will recommend an action (BERT model) based on the context and current state of the bandit.  We will assign a reward using the star ratings from Amazon Customer Reviews Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Client application generates a reward after receiving the recommended action and stores the tuple `<eventID, reward>` in S3. In this case, reward is 1 if predicted action is the true class, and 0 otherwise. SageMaker hosting endpoint saves all the inferences `<eventID, state, action, action probability>` to S3 using [**Kinesis Firehose**](https://aws.amazon.com/kinesis/data-firehose/). The `ExperimentManager` joins the reward with state, action and action probability using [**Amazon Athena**](https://aws.amazon.com/athena/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "local_mode = bandit_experiment_manager.local_mode\n",
    "\n",
    "num_events = 100 \n",
    "\n",
    "print('Generating {} sample events...'.format(num_events))\n",
    "\n",
    "for i in range(num_events):\n",
    "    context_index, context = client_app.choose_random_context()\n",
    "#    print('Context to predict = {}'.format(context))\n",
    "#    print('Context observation = [{}]'.format(context_index))    \n",
    "    action, event_id, bandit_model_id, action_prob, sample_prob = bandit_model.get_action(obs=[context_index])\n",
    "\n",
    "    reward = client_app.get_reward(context_index=context_index, \n",
    "                                   action=action, \n",
    "                                   event_id=event_id, \n",
    "                                   bandit_model_id=bandit_model_id, \n",
    "                                   action_prob=action_prob, \n",
    "                                   sample_prob=sample_prob, \n",
    "                                   local_mode=local_mode)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bandit Model Training Data\n",
    "\n",
    "Join `Event` and `Reward` data and upload to S3 in the following format:\n",
    "\n",
    "```\n",
    "{\n",
    " 'reward': -1, # -1 if the model is wrong, +1 if the model is correct\n",
    " 'event_id': 131181492351609994318271340276526219266, # unique event id\n",
    " 'action': 1, # suggested action (bert_model_id 1 or 2)\n",
    " 'action_prob': 0.9995, # probability that the suggested action is correct\n",
    " 'model_id': 'bandits-1597631299-model-id-1597631304', # unique bandit_model_id\n",
    " 'observation': [54], # feature (review_id)\n",
    " 'sample_prob': 0.43410828171830174 \n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if local_mode:\n",
    "    print('Using local mode with memory buffers.')\n",
    "    print()\n",
    "    print(client_app.joined_data_buffer)\n",
    "    bandit_experiment_manager.ingest_joined_data(client_app.joined_data_buffer)\n",
    "else:\n",
    "    print(\"Using production mode with Kinesis Firehose.  Waiting to flush to S3...\")\n",
    "    print()\n",
    "    time.sleep(60) # Wait for firehose to flush data to S3\n",
    "    rewards_s3_prefix = bandit_experiment_manager.ingest_rewards(client_app.rewards_buffer)\n",
    "    bandit_experiment_manager.join(rewards_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Experiment Status:  Joined\n",
    "`joining_state`:  `SUCCEEDED`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_experiment_manager._jsonify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Bandit Model Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bandit model training data {}'.format(bandit_experiment_manager.last_joined_job_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "bandit_model_train_data_s3_uri = S3Downloader.list(bandit_experiment_manager.last_joined_job_train_data)[0]\n",
    "print(bandit_model_train_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "bandit_model_train_data = S3Downloader.read_file(bandit_model_train_data_s3_uri)\n",
    "print(bandit_model_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Bandit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a new model with newly collected experiences, and host the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Trained bandit model id {}'.format(bandit_experiment_manager.last_trained_model_id))\n",
    "\n",
    "bandit_experiment_manager.train_next_model(input_data_s3_prefix=bandit_experiment_manager.last_joined_job_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore ^^ `Failed to delete` Error Above ^^ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Bandit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Deploying bandit model id {}'.format(bandit_experiment_manager.last_hosted_model_id))\n",
    "\n",
    "bandit_experiment_manager.deploy_model(model_id=bandit_experiment_manager.last_trained_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuously Train, Evaluate, and Deploy a New Bandit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a New Bandit Model\n",
    "The above cells explained the individual steps in the training workflow. To train a model to convergence, we will continually train the model based on data collected with client application interactions. We demonstrate the continual training loop in a single cell below.\n",
    "\n",
    "#### Evaluate New Bandit Model Against Current Bandit Model Before Deploying\n",
    "After every training cycle, we evaluate if the newly trained model (`last_trained_model_id`) would perform better than the one currently deployed (`last_hosted_model_id`) using a holdout evaluation dataset.  \n",
    "\n",
    "Details of the join, train, and evaluation steps are tracked in the `BanditsJoinTable` and `BanditsModelTable` DynamoDB tables.  When you have multiple experiments, you can compare them in the `BanditsExperimentTable` DynamoDB table.\n",
    "\n",
    "#### Deploy the New Bandit Model\n",
    "If the new bandit model is better than the current bandit model (based on offline evaluation), then we will automatically deploy the new bandit model using a blue-green deployment to avoid downtime. \n",
    "\n",
    "#### Monitor the Bandit Model in CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from markdown_helper import *\n",
    "from IPython.display import Markdown\n",
    "\n",
    "display(Markdown(bandit_experiment_manager.get_cloudwatch_dashboard_details()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "do_evaluation = True\n",
    "total_loops = 10 # Increase for higher accuracy\n",
    "retrain_batch_size = 100 # Model will be trained after every `batch_size` number of data instances\n",
    "rewards_list = []\n",
    "event_list = []\n",
    "\n",
    "local_mode = bandit_experiment_manager.local_mode\n",
    "for loop_no in range(total_loops):\n",
    "    print(f\"\"\"\n",
    "    #############\n",
    "    #### Loop {loop_no+1}\n",
    "    #############\n",
    "    \"\"\")\n",
    "    \n",
    "    # Generate experiences and log them\n",
    "    for i in range(retrain_batch_size):\n",
    "        context_index, context = client_app.choose_random_context()\n",
    "        action, event_id, bandit_model_id, action_prob, sample_prob = bandit_model.get_action(obs=[context_index])\n",
    "\n",
    "        print('*** Event ID {} ***'.format(event_id))\n",
    "        print('Context {}'.format(context_index))    \n",
    "        print('Recommended Action (BERT model) {}'.format(action))\n",
    "        print('Action Success Probability {}'.format(action_prob))\n",
    "        print('Action Sample Probability {}'.format(sample_prob))\n",
    "\n",
    "        reward = client_app.get_reward(context_index=context_index, \n",
    "                                       action=action, \n",
    "                                       event_id=event_id, \n",
    "                                       bandit_model_id=bandit_model_id, \n",
    "                                       action_prob=action_prob, \n",
    "                                       sample_prob=sample_prob, \n",
    "                                       local_mode=local_mode)\n",
    "\n",
    "        rewards_list.append(reward)  \n",
    "        \n",
    "    # Publish rewards sum for this batch to CloudWatch for monitoring \n",
    "    bandit_experiment_manager.cw_logger.publish_rewards_for_simulation(\n",
    "        bandit_experiment_manager.experiment_id,\n",
    "        sum(rewards_list[-retrain_batch_size:])/retrain_batch_size\n",
    "    )\n",
    "    \n",
    "    # Join the events and rewards data to use for the next bandit-model training job\n",
    "    # Use 90% as the training dataset and 10% as the the holdout evaluation dataset\n",
    "    if local_mode:        \n",
    "        bandit_experiment_manager.ingest_joined_data(client_app.joined_data_buffer,\n",
    "                                                     ratio=0.90)\n",
    "    else:\n",
    "        # Kinesis Firehose => S3 => Athena\n",
    "        print(\"Waiting for firehose to flush data to s3...\")\n",
    "        time.sleep(60) \n",
    "        rewards_s3_prefix = bandit_experiment_manager.ingest_rewards(client_app.rewards_buffer)\n",
    "        bandit_experiment_manager.join(rewards_s3_prefix, ratio=0.90)\n",
    "    \n",
    "    # Train \n",
    "    bandit_experiment_manager.train_next_model(\n",
    "        input_data_s3_prefix=bandit_experiment_manager.last_joined_job_train_data)\n",
    "\n",
    "    # Evaluate and/or deploy the new bandit model\n",
    "    if do_evaluation:\n",
    "        bandit_experiment_manager.evaluate_model(\n",
    "            input_data_s3_prefix=bandit_experiment_manager.last_joined_job_eval_data,\n",
    "            evaluate_model_id=bandit_experiment_manager.last_trained_model_id)\n",
    "\n",
    "        eval_score_last_trained_model = bandit_experiment_manager.get_eval_score(\n",
    "            evaluate_model_id=bandit_experiment_manager.last_trained_model_id,\n",
    "            eval_data_path=bandit_experiment_manager.last_joined_job_eval_data)\n",
    "\n",
    "        bandit_experiment_manager.evaluate_model(\n",
    "            input_data_s3_prefix=bandit_experiment_manager.last_joined_job_eval_data,\n",
    "            evaluate_model_id=bandit_experiment_manager.last_hosted_model_id)\n",
    "\n",
    "        eval_score_last_hosted_model = bandit_experiment_manager.get_eval_score(\n",
    "            evaluate_model_id=bandit_experiment_manager.last_hosted_model_id, \n",
    "            eval_data_path=bandit_experiment_manager.last_joined_job_eval_data)\n",
    "    \n",
    "        print('New bandit model evaluation score {}'.format(eval_score_last_hosted_model))\n",
    "        print('Current bandit model evaluation score {}'.format(eval_score_last_trained_model))\n",
    "\n",
    "        if eval_score_last_trained_model <= eval_score_last_hosted_model:\n",
    "            print('Deploying new bandit model id {} in loop {}'.format(bandit_experiment_manager.last_trained_model_id, loop_no))\n",
    "            bandit_experiment_manager.deploy_model(model_id=bandit_experiment_manager.last_trained_model_id)\n",
    "        else:\n",
    "            print('Not deploying bandit model id {} in loop {}'.format(bandit_experiment_manager.last_trained_model_id, loop_no))\n",
    "    else:\n",
    "        # Just deploy the new bandit model without evaluating against previous model\n",
    "        print('Deploying new bandit model id {} in loop {}'.format(bandit_experiment_manager.last_trained_model_id, loop_no))\n",
    "        bandit_experiment_manager.deploy_model(model_id=bandit_experiment_manager.last_trained_model_id)\n",
    "    \n",
    "    client_app.clear_buffer()\n",
    "    \n",
    "print(f\"Total time taken to complete {total_loops} loops: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Ignore Any Errors ^^ Above ^^_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Bandit Model Joined Event and Reward Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bandit model event and reward data {}'.format(bandit_experiment_manager.last_joined_job_eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "bandit_model_joined_event_and_reward_data_s3_uri = S3Downloader.list(bandit_experiment_manager.last_joined_job_eval_data)[0]\n",
    "print(bandit_model_joined_event_and_reward_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "bandit_model_joined_event_and_reward_data = S3Downloader.read_file(bandit_model_joined_event_and_reward_data_s3_uri)\n",
    "print(bandit_model_joined_event_and_reward_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Joined Event and Reward Data from S3 to Local Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_model_joined_event_and_reward_data_file_path = './'\n",
    "\n",
    "bandit_model_joined_event_and_reward_data = S3Downloader.download(bandit_model_joined_event_and_reward_data_s3_uri, bandit_model_joined_event_and_reward_data_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_model_joined_event_and_reward_data_local_file_path = bandit_model_joined_event_and_reward_data_s3_uri.split('/')[-1]\n",
    "\n",
    "df_joined_events_and_rewards = pd.read_csv(bandit_model_joined_event_and_reward_data_local_file_path, \n",
    "                                    delimiter=',', \n",
    "                                    quoting=csv.QUOTE_ALL)\n",
    "df_joined_events_and_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/dynamodb/home?region={}#tables:selected=BanditsExperimentTable;tab=items\">Bandits Experiment Table</a> in DynamoDB</b>'.format(region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Experiment State\n",
    "\n",
    "`evaluation_state`: `EVALUATED`\n",
    "\n",
    "The same bandit_model_id will appear in both `last_trained_model_id` and `last_evaluation_job_id` fields below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit_experiment_manager._jsonify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Bandit Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the bandit-model training performance by plotting the rolling mean reward across client interactions.\n",
    "\n",
    "Here rolling mean reward is calculated on the last `rolling_window` number of data instances, where each data instance corresponds to a single client interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_reward(reward_lst, batch_size=retrain_batch_size):\n",
    "    mean_rew=list()\n",
    "    for r in range(len(reward_lst)):\n",
    "        mean_rew.append(sum(reward_lst[:r+1]) * 1.0 / ((r+1)*retrain_batch_size))\n",
    "    return mean_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window = 100\n",
    "\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "lwd = 5\n",
    "cmap = plt.get_cmap('tab20')\n",
    "colors=plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "\n",
    "rewards_df = pd.DataFrame(rewards_list, columns=['bandit']).rolling(rolling_window).mean()\n",
    "rewards_df['perfect'] = sum(client_app.optimal_rewards) / len(client_app.optimal_rewards)\n",
    "\n",
    "rewards_df.plot(y=['bandit', 'perfect'], \n",
    "                linewidth=lwd)\n",
    "plt.legend(loc=4, prop={'size': 20})\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.yticks([0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1.00])\n",
    "plt.xticks([100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])\n",
    "\n",
    "plt.xlabel('Data instances (bandit model is updated every %s data instances)' % retrain_batch_size, size=20)\n",
    "plt.ylabel('Rolling {} Mean Reward'.format(rolling_window), size=30)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Invocation Metrics for the BERT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "    \n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#metricsV2:namespace=AWS/SageMaker;dimensions=EndpointName,VariantName;search={}\">Model 1 SageMaker REST Endpoint</a></b>'.format(region, model_1_endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#metricsV2:namespace=AWS/SageMaker;dimensions=EndpointName,VariantName;search={}\">Model 2 SageMaker REST Endpoint</a></b>'.format(region, model_2_endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the Reward Data Across All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_df.bandit.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three DynamoDB tables from the bandits application above (e.g. `experiment_id='bandits-...'`). To better maintain them, we should remove the related records if the experiment has finished. \n",
    "\n",
    "Only execute the clean up cells below when you've finished the current experiment and want to deprecate everything associated with it. \n",
    "\n",
    "_The CloudWatch metrics will be removed during this cleanup step._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning up experiment_id {}'.format(bandit_experiment_manager.experiment_id))\n",
    "try:\n",
    "    bandit_experiment_manager.clean_resource(experiment_id=bandit_experiment_manager.experiment_id)\n",
    "    bandit_experiment_manager.clean_table_records(experiment_id=bandit_experiment_manager.experiment_id)\n",
    "except:\n",
    "    print('Ignore any errors.  Errors are OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "550.4px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
